{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gu-ma/hgk-ml-workshop/blob/main/notebooks/Image_Search_01_Process_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oofwFq2xVJl9"
      },
      "source": [
        "# Process your dataset with CLIP\n",
        "\n",
        "This notebook processes all the downloaded photos using OpenAI's [CLIP neural network](https://github.com/openai/CLIP). For each image we get a feature vector containing 512 float numbers, which we will store in a file. These feature vectors will be used later to compare them to the text feature vectors.\n",
        "\n",
        "This step will be significantly faster if you have a GPU, but it will also work on the CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "lxtCTguibfaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "JsQRhWYIY5pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Load the open CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Function that computes the feature vectors for a batch of images\n",
        "def compute_clip_features(photos_batch):\n",
        "    # Load all the photos from the files\n",
        "    photos = [Image.open(photo_file) for photo_file in photos_batch]\n",
        "    \n",
        "    # Preprocess all photos\n",
        "    photos_preprocessed = torch.stack([preprocess(photo) for photo in photos]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Encode the photos batch to compute the feature vectors and normalize them\n",
        "        photos_features = model.encode_image(photos_preprocessed)\n",
        "        photos_features /= photos_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Transfer the feature vectors back to the CPU and convert to numpy\n",
        "    return photos_features.cpu().numpy()"
      ],
      "metadata": {
        "id": "DbJOGduzgB_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Gdrive"
      ],
      "metadata": {
        "id": "9rMatyaoVlhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D-MxgnGBVoM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI7Oa1i4VJl-"
      },
      "source": [
        "## Process the photos\n",
        "\n",
        "Load all photos from the folder they were stored. We will then compute the features for all photos. We will do that in batches, because it is much more efficient. You should tune the batch size so that it fits on your GPU. The processing on the GPU is fairly fast, so the bottleneck will probably be loading the photos from the disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HwHzJP6VJl-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import math\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown Path to source directory on google drive. Right click your directory and choose \"copy path\" then paste it below.\n",
        "\n",
        "# @markdown ⚠️ __This is the folder with the scenes and images, not the original videos dataset__ ⚠️\n",
        "gdrive_input_dir = \"/content/drive/MyDrive/AI/hgk_workshop/playlist01_output\"  # @param { type:'string' }\n",
        "\n",
        "# @markdown Must be smaller than the number of images\n",
        "batch_size = 32  # @param { type:\"number\" }\n",
        "\n",
        "# @markdown Copy all files to google drive when done\n",
        "copy_to_gdrive = True  # @param { type:\"boolean\" }\n",
        "\n",
        "# Some other dir / vars\n",
        "(gdrive_path, gdrive_folder) = os.path.split(gdrive_input_dir)\n",
        "\n",
        "input_dir = gdrive_folder\n",
        "output_dir = f'{gdrive_folder}_clip'\n",
        "\n",
        "gdrive_output_dir = os.path.join(gdrive_path, output_dir)\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(input_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Copy all jpg locally\n",
        "! cp -r {gdrive_input_dir}/*.jpg {input_dir}\n",
        "\n",
        "# Delete existing features\n",
        "! rm {output_dir}/*.*\n",
        "\n",
        "\n",
        "# Set the path to the photos\n",
        "photos_path = Path(input_dir)\n",
        "\n",
        "# List all JPGs in the folder\n",
        "photos_files = list(photos_path.glob(\"*.jpg\"))\n",
        "photos_files.sort()\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"Photos found: {len(photos_files)}\")\n",
        "print(*photos_files[:10], sep='\\n')\n",
        "\n",
        "# Path where the feature vectors will be stored\n",
        "features_path = Path(output_dir)\n",
        "\n",
        "# Compute how many batches are needed\n",
        "batches = math.ceil(len(photos_files) / batch_size)\n",
        "\n",
        "# Process each batch\n",
        "for i in range(batches):\n",
        "    print(f\"Processing batch {i+1}/{batches}\")\n",
        "\n",
        "    batch_ids_path = features_path / f\"{i:010d}.csv\"\n",
        "    batch_features_path = features_path / f\"{i:010d}.npy\"\n",
        "    \n",
        "    # Only do the processing if the batch wasn't processed yet\n",
        "    if not batch_features_path.exists():\n",
        "        try:\n",
        "            # Select the photos for the current batch\n",
        "            batch_files = photos_files[i*batch_size : (i+1)*batch_size]\n",
        "\n",
        "            # Compute the features and save to a numpy file\n",
        "            batch_features = compute_clip_features(batch_files)\n",
        "            np.save(batch_features_path, batch_features)\n",
        "\n",
        "            # Save the photo IDs to a CSV file\n",
        "            photo_ids = [photo_file.name.split(\".\")[0] for photo_file in batch_files]\n",
        "            photo_ids_data = pd.DataFrame(photo_ids, columns=['photo_id'])\n",
        "            photo_ids_data.to_csv(batch_ids_path, index=False)\n",
        "        except:\n",
        "            # Catch problems with the processing to make the process more robust\n",
        "            print(f'Problem with batch {i}')\n",
        "\n",
        "# Merge the features and the photo IDs. \n",
        "\n",
        "# Load all numpy files\n",
        "features_list = [np.load(features_file) for features_file in sorted(features_path.glob(\"*.npy\"))]\n",
        "\n",
        "# Concatenate the features and store in a merged file\n",
        "features = np.concatenate(features_list)\n",
        "np.save(features_path / \"features.npy\", features)\n",
        "\n",
        "# Load all the photo IDs\n",
        "photo_ids = pd.concat([pd.read_csv(ids_file) for ids_file in sorted(features_path.glob(\"*.csv\"))])\n",
        "photo_ids.to_csv(features_path / \"photo_ids.csv\", index=False)\n",
        "\n",
        "# Delete inter results \n",
        "! rm {features_path}/*[0-9]*.*\n",
        "\n",
        "# Copy result to gdrive\n",
        "if copy_to_gdrive:\n",
        "    shutil.copytree(output_dir, gdrive_output_dir, dirs_exist_ok=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lxtCTguibfaw",
        "9rMatyaoVlhQ"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}