{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tjnMLJwRUK8W",
        "pTABQ778UiZl",
        "4M4h8SHuUm-r",
        "IVubwpVRmMH-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gu-ma/hgk-ml-workshop/blob/main/notebooks/train_gpt2(aitextgen).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Train a GPT-2 (or GPT Neo) Text-Generating Model w/ GPU\n",
        "\n",
        "original colab by [Max Woolf](https://minimaxir.com)\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Colaboratory** using `aitextgen`!\n",
        "\n",
        "For more about `aitextgen`, you can visit [this GitHub repository](https://github.com/minimaxir/aitextgen) or [read the documentation](https://docs.aitextgen.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "tjnMLJwRUK8W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_"
      },
      "source": [
        "# !pip install -q aitextgen\n",
        "!pip install git+https://github.com/llimllib/aitextgen@fix_tpu_available \n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO\n",
        "    )\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive, copy_file_to_gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect Google drive"
      ],
      "metadata": {
        "id": "pTABQ778UiZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aYuM9P7FUlsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Try some models"
      ],
      "metadata": {
        "id": "4M4h8SHuUm-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Choose and Load model\n",
        "\n",
        "# @markdown Select a model to load\n",
        "model = \"huggingartists/eminem\" #@param [\"striki-ai/william-shakespeare-poetry\",\"huggingartists/eminem\",\"pranavpsv/gpt2-genre-story-generator\",\"fabianmmueller/deep-haiku-gpt-2\"] {allow-input: true}\n",
        "\n",
        "# @markdown If you feel aventurous you can also try some models [from here](https://huggingface.co/models?library=transformers,pytorch&language=en&other=gpt2&p=1&sort=downloads) just use the sufix of the url. For example huggingface.co/__huggingartists/rihanna__\n",
        "\n",
        "ai = aitextgen(model=model, to_gpu=True)\n",
        "\n",
        "print(f\"---\\nRead the model card for info on how to generate text https://huggingface.co/{model}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jE9y8levNGCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate some samples\n",
        "\n",
        "# @markdown How the text should start\n",
        "prompt = \"I am a\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown Number of samples to generate\n",
        "n = 4  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Length of the generated text\n",
        "max_length = 64  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with top_p=0.9)\n",
        "top_p = 0.9  # @param {type:\"number\"}\n",
        "\n",
        "ai.generate(\n",
        "    n=n, prompt=prompt, max_length=max_length, temperature=temperature, top_p=top_p\n",
        ")\n"
      ],
      "metadata": {
        "id": "bNHGgDzdNSmg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Finetune GPT-2"
      ],
      "metadata": {
        "id": "IVubwpVRmMH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First upload a file to Colab and set the path. \n",
        "\n",
        "If your text file is large (>10MB), it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM.\n",
        "\n",
        "Additionally, you may want to consider [compressing the dataset to a cache first](https://docs.aitextgen.io/dataset/) on your local computer, then uploading the resulting `dataset_cache.tar.gz` and setting the `file_name`in the previous cell to that."
      ],
      "metadata": {
        "id": "86XryFYvWufv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"warpeace_input.txt\""
      ],
      "metadata": {
        "id": "0aRdMph0mKye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "The next cell will start the actual finetuning of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf"
      },
      "source": [
        "ai.train(\n",
        "    file_name,\n",
        "    line_by_line=False,\n",
        "    from_cache=False,\n",
        "    num_steps=3000,\n",
        "    generate_every=1000,\n",
        "    save_every=1000,\n",
        "    save_gdrive=False,\n",
        "    learning_rate=1e-3,\n",
        "    fp16=False,\n",
        "    batch_size=1,\n",
        ")\n",
        "model_folder = \"trained_model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model. But first it might be a good idea to copy you model to gdrive ⬇️"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload model to Google Drive\n",
        "\n",
        "gdrive_output_dir = \"/content/drive/MyDrive/AI/hgk_workshop\" #@param {type:\"string\"}\n",
        "\n",
        "model_name = \"my-gpt2-model\" #@param {type:\"string\"}\n",
        "\n",
        "dst = Path(gdrive_output_dir) / Path(model_name)\n",
        "\n",
        "src = '/content/trained_model/'\n",
        "\n",
        "# Create dst dir if it does not exist\n",
        "if not os.path.isdir(dst):\n",
        "    os.mkdir(dst)\n",
        "\n",
        "# Copy the processed files to gdrive\n",
        "print(f'Copying files to Google Drive this could take some time 😴')\n",
        "shutil.copytree(src, dst, dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "yyTj71dGIkSC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "\n",
        "#3. Load a Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you already had a trained model from this notebook, running the next cell will copy the `pytorch_model.bin` and the `config.json`file from the specified folder in Google Drive into the Colaboratory VM."
      ],
      "metadata": {
        "id": "6safv8ZEW0QS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "cellView": "form"
      },
      "source": [
        "#@title Download model from Google Drive\n",
        "\n",
        "# @markdown Path to your model on google drive. Right click your directory and choose \"copy path\" then paste it below\n",
        "gdrive_model_dir = \"/content/drive/MyDrive/AI/hgk_workshop/my-gpt2-model\" #@param {type:\"string\"}\n",
        "\n",
        "(gdrive_path, model_name) = os.path.split(gdrive_model_dir)\n",
        "\n",
        "src = gdrive_model_dir\n",
        "\n",
        "dst = f'/content/{model_name}/'\n",
        "model_folder = dst\n",
        "\n",
        "# Create local dst dir if it does not exist\n",
        "if not os.path.isdir(dst):\n",
        "    os.mkdir(dst)\n",
        "\n",
        "# Copy the processed files to gdrive\n",
        "print(f'Copying files from Google Drive')\n",
        "shutil.copytree(src, dst, dirs_exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "**If you just trained a model**, you'll get much faster training performance if you reload the model; the next cell will reload the model you just trained from the `trained_model` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSvHhTuHJc-Q"
      },
      "source": [
        "ai = aitextgen(model_folder=model_folder, to_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd0RGDbJiDp"
      },
      "source": [
        "`generate()` without any parameters generates a single text from the loaded model to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL"
      },
      "source": [
        "ai.generate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "You can also pass in a `prompt` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 50 for `batch_size` to avoid going OOM).\n",
        "\n",
        "Other optional-but-helpful parameters for `ai.generate()` and friends:\n",
        "\n",
        "*  **`min length`**: The minimum length of the generated text: if the text is shorter than this value after cleanup, aitextgen will generate another one.\n",
        "*  **`max_length`**: Number of tokens to generate (default 256, you can generate up to 1024 tokens with GPT-2 and 2048 with GPT Neo)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "ai.generate(\n",
        "    n=5,\n",
        "    prompt=\"A man\",\n",
        "    max_length=128,\n",
        "    temperature=1.0,\n",
        "    top_p=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of texts to a file and sort out the samples locally on your computer. The next cell will generate `num_files` files, each with `n` texts and whatever other parameters you would pass to `generate()`. The files can then be downloaded from the Files sidebar!\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "num_files = 5\n",
        "\n",
        "for _ in range(num_files):\n",
        "    ai.generate_to_file(\n",
        "        n=10,\n",
        "        prompt=\"A man\",\n",
        "        max_length=256,\n",
        "        temperature=1.0,\n",
        "        top_p=0.9\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zip text files and delete them"
      ],
      "metadata": {
        "id": "mhs8DMmQWam9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! zip samples.zip *.txt\n",
        "! rm *.txt"
      ],
      "metadata": {
        "id": "_L9At4GfWVtj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}